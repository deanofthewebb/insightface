# recognition/arcface_torch/configs/custom_onnx_ft.py
from easydict import EasyDict as edict

cfg = edict()

# ------------------------------------------------------------------
# Basic setup
# ------------------------------------------------------------------
cfg.output = "work_dirs/custom_onnx_ft"   # where logs & checkpoints go
cfg.resume = False
cfg.fp16 = True                          # mixed precision
cfg.seed = 42

# ------------------------------------------------------------------
# Dataset (InsightFace "rec" format)
# ------------------------------------------------------------------
# Path to your shuffled rec dataset folder, e.g. generated by
# scripts/shuffle_rec.py in the repo.
# It should contain train.rec / train.idx and val.rec / val.idx (optional).
cfg.rec = "your_dataset_rec"   ### TODO: set this

# Number of identities (classes) in your dataset
cfg.num_classes = 127          ### TODO: set this

# Total *training* images in your rec
# (used only to compute number of steps; doesn't need to be exact)
cfg.num_image = cfg.num_classes * 20   ### TODO: adjust if you know exact count

# Validation targets: if you don't have LFW/CFP/AgeDB recs set up,
# you can leave this empty and train_v3 will skip verification.
cfg.val_targets = []  # e.g. ["lfw", "cfp_fp", "agedb_30"]

# ------------------------------------------------------------------
# Training schedule
# ------------------------------------------------------------------
cfg.batch_size = 64           # per-process (i.e. per GPU) batch size
cfg.num_workers = 4
cfg.dali = False              # leave False unless you set up NVIDIA DALI

cfg.num_epoch = 20            # fine‑tuning; usually much smaller than full training
cfg.warmup_epoch = 0          # no LR warmup for small fine‑tune
cfg.optimizer = "adamw"       # good default for fine‑tuning
cfg.lr = 1e-4                 # conservative LR for fine‑tuning
cfg.weight_decay = 5e-4
cfg.momentum = 0.9            # used only if you switch optimizer to sgd

# ------------------------------------------------------------------
# ArcFace / PartialFC hyper‑params
# ------------------------------------------------------------------
cfg.embedding_size = 512      # must match ONNXArcFaceBackbone output; we override if needed

# margin_list = [m1, m2, m3] for CombinedMarginLoss:
#   m1: l_a, multiplicative angular margin
#   m2: l_m, additive angular margin (classic ArcFace ≈ 0.5)
#   m3: l_c, additive cosine margin (often 0)
cfg.margin_list = [1.0, 0.5, 0.0]

# If you want to ignore extremely hard negatives, you can set this > 0.
cfg.interclass_filtering_threshold = 0.0

# PartialFC sampling rate: 1.0 = no sampling (small num_classes is fine)
cfg.sample_rate = 1.0

# ------------------------------------------------------------------
# Logging / saving
# ------------------------------------------------------------------
cfg.frequent = 10            # log training loss every N steps
cfg.verbose = 2000           # run verification every N steps (if val_targets)
cfg.gradient_acc = 1         # gradient accumulation steps

cfg.save_all_states = True
cfg.save_interval = 5000     # steps between full checkpoint saves

# The following keys are used by train_v3 but set programmatically:
#   total_batch_size, warmup_step, total_step
# Do not set them here.
