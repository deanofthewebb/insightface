{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ReidNet V3 Training - InsightFace ArcFace with ImageFolder Dataset\n",
    "\n",
    "PyTorch-based ArcFace training with natural image directory structure.\n",
    "\n",
    "**Environment:**\n",
    "- GPU runtime (A100/V100/T4) with CUDA 12.x\n",
    "- ~50GB disk space for dataset\n",
    "- Python 3.8+ (any version)\n",
    "- Root directory: `/home/ubuntu/`\n",
    "\n",
    "**Workflow:**\n",
    "1. Run Step 1 (environment check)\n",
    "2. Run Step 2 (install dependencies) \u2192 **Restart kernel**\n",
    "3. Run Step 2b and continue sequentially\n",
    "\n",
    "**Dataset Format:**\n",
    "- ImageFolder: `dataset_root/<identity_id>/<image_files>`\n",
    "- Images are 112x112 RGB JPEG\n",
    "- Auto-detected: num_classes = number of identity subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_check"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udd0d STEP 1: ENVIRONMENT CHECK\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\ud83d\udd0d ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "\n",
    "# Setup directories\n",
    "WORKDIR = Path(\"/home/ubuntu/insightface_training\")\n",
    "CHECKPOINT_DIR = Path(\"/home/ubuntu/checkpoints/reidnet_v3\")\n",
    "\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc2 Working directory: {WORKDIR}\")\n",
    "print(f\"\ud83d\udcbe Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n\ud83c\udfae GPU CHECK\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udce6 STEP 2: INSTALL DEPENDENCIES\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"\ud83d\udce6 INSTALLING DEPENDENCIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix PATH for local binaries\n",
    "import os\n",
    "if '/home/ubuntu/.local/bin' not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = f\"/home/ubuntu/.local/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Ensure pip is available in current Python environment\n",
    "print(\"\ud83d\udd27 Ensuring pip is available...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"--version\"], capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"\u26a0\ufe0f  pip not found, bootstrapping...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--default-pip\"], check=True)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], check=True)\n",
    "print(\"\u2705 pip ready\")\n",
    "\n",
    "# Core dependencies\n",
    "!{sys.executable} -m pip install -q numpy boto3 awscli\n",
    "\n",
    "# PyTorch ecosystem (CUDA 12.1 for CUDA 12.x systems)\n",
    "!{sys.executable} -m pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Training dependencies (NO mxnet - PyTorch has native RecordIO reader)\n",
    "!{sys.executable} -m pip install -q tensorboard onnx onnxruntime-gpu easydict opencv-python scikit-image scikit-learn tqdm\n",
    "\n",
    "print(\"\\n\u2705 Dependencies installed\")\n",
    "print(\"\\n\u26a0\ufe0f  IMPORTANT: Restart kernel now (Kernel \u2192 Restart)\")\n",
    "print(\"    Then skip this cell and continue from Step 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_packages"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udccb STEP 2b: VERIFY PACKAGES (AFTER KERNEL RESTART)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix PATH\n",
    "if '/home/ubuntu/.local/bin' not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = f\"/home/ubuntu/.local/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\ud83d\udccb PACKAGE VERSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  CUDA not available - training will be very slow!\")\n",
    "\n",
    "# Re-establish paths from Step 1\n",
    "WORKDIR = Path(\"/home/ubuntu/insightface_training\")\n",
    "CHECKPOINT_DIR = Path(\"/home/ubuntu/checkpoints/reidnet_v3\")\n",
    "print(f\"\\n\u2705 Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udce5 STEP 3: CLONE INSIGHTFACE REPOSITORY\n",
    "import os\n",
    "\n",
    "repo_path = WORKDIR / \"insightface\"\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')\n",
    "GITHUB_USER = os.getenv('GITHUB_USER', 'deanofthewebb')\n",
    "BRANCH = \"main\"\n",
    "\n",
    "if GITHUB_TOKEN:\n",
    "    REPO_URL = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/insightface.git\"\n",
    "else:\n",
    "    REPO_URL = f\"https://github.com/{GITHUB_USER}/insightface.git\"\n",
    "\n",
    "if not repo_path.exists():\n",
    "    print(f\"\ud83d\udce5 Cloning InsightFace repository...\")\n",
    "    !git clone --depth 1 --branch {BRANCH} {REPO_URL} {repo_path}\n",
    "    print(\"\u2705 Repository cloned\")\n",
    "else:\n",
    "    print(\"\ud83d\udcc1 Repository exists, pulling latest changes...\")\n",
    "    !cd {repo_path} && git pull origin {BRANCH}\n",
    "    print(\"\u2705 Repository updated\")\n",
    "\n",
    "# Navigate to training directory\n",
    "training_dir = repo_path / \"recognition\" / \"arcface_torch\"\n",
    "os.chdir(training_dir)\n",
    "print(f\"\\n\ud83d\udccd Working in: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aws_credentials"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udd11 STEP 4: CONFIGURE AWS CREDENTIALS\n",
    "import os\n",
    "import pathlib\n",
    "import boto3\n",
    "\n",
    "print(\"\ud83d\udd11 CONFIGURING AWS CREDENTIALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read credentials from environment\n",
    "AK = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "SK = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "REGION = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-2\")\n",
    "\n",
    "if not AK or not SK:\n",
    "    raise ValueError(\"AWS credentials not found. Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\")\n",
    "\n",
    "# Clear stale session tokens (keep credentials)\n",
    "for k in [\"AWS_SESSION_TOKEN\", \"AWS_SECURITY_TOKEN\", \"AWS_PROFILE\"]:\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "# Write AWS config\n",
    "aws_dir = pathlib.Path.home() / \".aws\"\n",
    "aws_dir.mkdir(parents=True, exist_ok=True)\n",
    "(aws_dir / \"credentials\").write_text(\n",
    "    f\"[default]\\naws_access_key_id={AK}\\naws_secret_access_key={SK}\\n\"\n",
    ")\n",
    "(aws_dir / \"config\").write_text(\n",
    "    f\"[default]\\nregion={REGION}\\noutput=json\\n\"\n",
    ")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AK\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = SK\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = REGION\n",
    "\n",
    "# Verify credentials\n",
    "try:\n",
    "    sts = boto3.client(\"sts\", region_name=REGION)\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"\\n\u2705 AWS credentials verified\")\n",
    "    print(f\"   Account: {identity['Account']}\")\n",
    "    print(f\"   User: {identity['Arn']}\")\n",
    "    \n",
    "    # Check disk space\n",
    "    print(\"\\n\ud83d\udcca Disk Space:\")\n",
    "    !df -h /home/ubuntu | head -2\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c AWS verification failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_dataset"
   },
   "outputs": [],
   "source": "# \ud83d\udce5 STEP 5: DOWNLOAD DATASET ZIP FROM S3\nimport subprocess\nimport zipfile\nfrom tqdm import tqdm\n\nprint(\"\ud83d\udce5 REIDNET V3 DATASET PREPARATION\")\nprint(\"=\" * 60)\n\n# Dataset configuration\nBUCKET = \"data-labeling.livereachmedia.com\"\nZIP_KEY = \"datasets/reidnet_v3/reidnet_v3_processed.zip\"\nZIP_PATH = WORKDIR / \"reidnet_v3_processed.zip\"\nDATASET_DIR = WORKDIR / \"datasets\" / \"reidnet_v3_imagefolder\"\n\n# Check if dataset already extracted\nif DATASET_DIR.exists() and len(list(DATASET_DIR.iterdir())) > 100:\n    print(f\"\\n\u2705 Dataset already exists at: {DATASET_DIR}\")\n    print(\"   Skipping download and extraction.\")\nelse:\n    print(f\"\\n\ud83d\udce5 Downloading dataset zip from S3...\")\n    print(f\"   Source: s3://{BUCKET}/{ZIP_KEY}\")\n    print(f\"   Destination: {ZIP_PATH}\")\n    print(f\"   Size: ~1.8 GB (this will take 5-10 minutes)\\n\")\n    \n    # Download zip using AWS CLI\n    result = subprocess.run([\n        \"aws\", \"s3\", \"cp\",\n        f\"s3://{BUCKET}/{ZIP_KEY}\",\n        str(ZIP_PATH)\n    ], capture_output=True, text=True)\n    \n    if result.returncode != 0:\n        print(f\"\u274c Download failed: {result.stderr}\")\n        raise RuntimeError(\"Dataset download failed\")\n    \n    print(f\"\\n\u2705 Download complete\")\n    \n    # Extract zip\n    print(f\"\\n\ud83d\udce6 Extracting dataset...\")\n    print(f\"   This will take 5-10 minutes...\\n\")\n    \n    DATASET_DIR.parent.mkdir(parents=True, exist_ok=True)\n    \n    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n        # Extract to parent directory (zip contains reidnet_v3_processed/ folder)\n        zip_ref.extractall(DATASET_DIR.parent)\n    \n    # Rename extracted folder to expected name\n    extracted_dir = DATASET_DIR.parent / \"reidnet_v3_processed\"\n    if extracted_dir.exists():\n        extracted_dir.rename(DATASET_DIR)\n    \n    print(f\"\\n\u2705 Extraction complete\")\n    \n    # Remove zip to save space\n    print(f\"\\n\ud83e\uddf9 Cleaning up zip file...\")\n    ZIP_PATH.unlink(missing_ok=True)\n\n# Count identities and images\nimport os\n\nprint(\"\\n\ud83d\udcca Analyzing dataset...\")\nidentity_dirs = [d for d in DATASET_DIR.iterdir() if d.is_dir()]\nnum_identities = len(identity_dirs)\n\n# Count images in sample for estimation\nsample_size = min(1000, num_identities)\nprint(f\"   Sampling {sample_size} identities...\")\n\nimage_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\nsample_total = 0\nfor identity_dir in tqdm(identity_dirs[:sample_size], desc=\"   Counting images\", leave=False):\n    sample_total += sum(1 for f in identity_dir.iterdir() \n                       if f.is_file() and f.suffix.lower() in image_extensions)\n\navg_images_per_identity = sample_total / sample_size if sample_size > 0 else 0\nestimated_total = int(avg_images_per_identity * num_identities)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\ud83d\udcca DATASET STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Identities: {num_identities:,}\")\nprint(f\"Estimated total images: {estimated_total:,}\")\nprint(f\"Avg images/identity: {avg_images_per_identity:.1f}\")\nprint(f\"\\n\u2705 Dataset ready at: {DATASET_DIR}\")\n\n# Verify no RecordIO files exist (should use ImageFolder format)\nrec_file = DATASET_DIR / \"train.rec\"\nidx_file = DATASET_DIR / \"train.idx\"\nif rec_file.exists() or idx_file.exists():\n    print(\"\\n\u26a0\ufe0f  WARNING: Found RecordIO files (train.rec/train.idx)\")\n    print(\"   Removing them to ensure ImageFolder format is used...\")\n    rec_file.unlink(missing_ok=True)\n    idx_file.unlink(missing_ok=True)\n    print(\"   \u2705 RecordIO files removed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_pretrained"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udce5 STEP 6: DOWNLOAD PRETRAINED MODEL\n",
    "print(\"\ud83d\udce5 DOWNLOADING PRETRAINED ONNX MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pretrained_dir = WORKDIR / \"pretrained_models\"\n",
    "pretrained_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NVR production ONNX backbone (LResNet100E-IR ArcFace)\n",
    "# Note: Upload nvr.prod.v7.onnx to S3 if not already there\n",
    "S3_MODEL_PATH = \"s3://data-labeling.livereachmedia.com/datasets/face_rec/nvr.prod.v7.onnx\"\n",
    "model_name = \"nvr.prod.v7.facerec.backbone.onnx\"\n",
    "local_model = pretrained_dir / model_name\n",
    "\n",
    "if local_model.exists():\n",
    "    print(f\"\\n\u23ed\ufe0f  Model already exists: {model_name}\")\n",
    "    ONNX_BACKBONE = str(local_model)\n",
    "else:\n",
    "    try:\n",
    "        print(f\"\\n\ud83d\udce6 Downloading {model_name}...\")\n",
    "        !aws s3 cp {S3_MODEL_PATH} {local_model} --only-show-errors\n",
    "        \n",
    "        size_mb = local_model.stat().st_size / 1e6\n",
    "        print(f\"\u2705 Downloaded ({size_mb:.1f} MB)\")\n",
    "        ONNX_BACKBONE = str(local_model)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u26a0\ufe0f  Failed to download ONNX model: {e}\")\n",
    "        print(\"   Training will start with random initialization\")\n",
    "        ONNX_BACKBONE = None\n",
    "\n",
    "print(f\"\\n\ud83d\udccd ONNX backbone: {ONNX_BACKBONE or 'None (random init)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [],
   "source": "# \u2699\ufe0f STEP 7: CREATE TRAINING CONFIG\nprint(\"\u2699\ufe0f CREATING TRAINING CONFIG\")\nprint(\"=\" * 60)\n\n# Count identities from dataset directory\nidentity_dirs = [d for d in DATASET_DIR.iterdir() if d.is_dir()]\nnum_identities = len(identity_dirs)\n\n# Count total images (sample based)\ntotal_images = 0\nfor identity_dir in identity_dirs[:100]:\n    total_images += len(list(identity_dir.glob('*.jpg'))) + len(list(identity_dir.glob('*.png')))\navg_images = total_images / min(100, num_identities) if num_identities > 0 else 0\nestimated_total_images = int(avg_images * num_identities)\n\nprint(f\"\\nDataset: {num_identities:,} identities, ~{estimated_total_images:,} images\")\n\n# Create config\nconfig_content = f'''# ReidNet V3 Fine-tuning Configuration (ImageFolder)\nfrom easydict import EasyDict as edict\n\nconfig = edict()\n\n# Network architecture\nconfig.network = \"r100\"  # ResNet100 backbone (ignored when using ONNX)\nconfig.embedding_size = 512\nconfig.margin_list = (1.0, 0.5, 0.0)  # ArcFace (m, s, a)\nconfig.interclass_filtering_threshold = 0.0\n\n# Output directory\nconfig.output = \"{CHECKPOINT_DIR / 'work_dirs'}\"\nconfig.resume = False\n\n# Dataset (ImageFolder format - auto-detects num_classes)\nconfig.rec = \"{DATASET_DIR}\"\nconfig.num_classes = {num_identities}  # Auto-detected from folders\nconfig.num_image = {estimated_total_images}  # Estimated\nconfig.num_workers = 4  # Reduced for 15GB GPU\nconfig.dali = False\nconfig.dali_aug = False\n\n# Training hyperparameters\nconfig.batch_size = 32  # Optimized for 15GB GPU (T4)\nconfig.lr = 0.01  # Conservative for fine-tuning\nconfig.optimizer = \"sgd\"\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.sample_rate = 1.0\nconfig.fp16 = True\n\n# Training schedule\nconfig.num_epoch = 24\nconfig.warmup_epoch = 0\n\n# Logging and checkpointing\nconfig.verbose = 2000\nconfig.frequent = 20\nconfig.save_all_states = True\nconfig.save_interval = 20000\nconfig.gradient_acc = 1\n\n# Evaluation\nconfig.val_targets = []\n\nconfig.seed = 2048\n'''\n\n# Write config\nconfig_dir = training_dir / \"configs\"\nconfig_dir.mkdir(exist_ok=True)\nconfig_file = config_dir / \"reidnet_v3_imagefolder.py\"\nconfig_file.write_text(config_content)\n\nprint(f\"\\n\u2705 Config saved: {config_file}\")\nprint(\"\\n\ud83d\udccb Training Configuration:\")\nprint(f\"   Identities: {num_identities:,}\")\nprint(f\"   Images: ~{estimated_total_images:,}\")\nprint(f\"   Batch size: 32 per GPU (15GB GPU)\")\nprint(f\"   Epochs: 24\")\nprint(f\"   Mixed precision: Enabled\")\nprint(f\"   Pretrained: {'Yes' if ONNX_BACKBONE else 'No (from scratch)'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# \ud83d\ude80 STEP 8: START TRAINING\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"\ud83d\ude80 STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "os.chdir(training_dir)\n",
    "\n",
    "# Detect number of GPUs\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "if num_gpus == 0:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: No GPUs detected! Training will be extremely slow.\")\n",
    "    print(\"   Please ensure you're running on a GPU instance.\\n\")\n",
    "    # Use CPU fallback with current Python interpreter\n",
    "    cmd = [sys.executable, \"train_v3.py\", \"configs/reidnet_v3_imagefolder.py\"]\n",
    "else:\n",
    "    print(f\"\\n\ud83c\udfae Detected {num_gpus} GPU(s)\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Use torchrun for distributed training with current Python interpreter\n",
    "    # torchrun will use sys.executable by default, but we can also specify it via PATH\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"torch.distributed.run\",\n",
    "        f\"--nproc_per_node={num_gpus}\",\n",
    "        \"--standalone\",\n",
    "        \"train_v3.py\",\n",
    "        \"configs/reidnet_v3_imagefolder.py\"\n",
    "    ]\n",
    "\n",
    "# Add ONNX backbone if available\n",
    "if ONNX_BACKBONE and Path(ONNX_BACKBONE).exists():\n",
    "    cmd.extend([\"--onnx-backbone\", ONNX_BACKBONE])\n",
    "    print(f\"\\n\ud83d\udce6 Using ONNX backbone:\\n   {ONNX_BACKBONE}\")\n",
    "    print(f\"   Architecture: LResNet100E-IR (512-D embeddings)\")\n",
    "else:\n",
    "    print(\"\\n\ud83d\udd28 Training from random initialization (no pretrained backbone)\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbb Python: {sys.executable}\")\n",
    "print(f\"\ud83d\udcbb Command: {' '.join(cmd)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udfaf TRAINING STARTED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n\ud83d\udcc2 Checkpoints: {CHECKPOINT_DIR / 'work_dirs'}\")\n",
    "print(\"\\nTo monitor: Open TensorBoard in next cell\")\n",
    "print(\"To stop: Runtime \u2192 Interrupt\\n\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Execute training\n",
    "!{' '.join(cmd)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor_training"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udcca STEP 9: MONITOR TRAINING (RUN IN PARALLEL)\n",
    "print(\"\ud83d\udcca LAUNCHING TENSORBOARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "%load_ext tensorboard\n",
    "tensorboard_dir = CHECKPOINT_DIR / \"work_dirs\" / \"logs\"\n",
    "print(f\"\\n\ud83d\udcc8 Log directory: {tensorboard_dir}\\n\")\n",
    "\n",
    "%tensorboard --logdir {tensorboard_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_model"
   },
   "outputs": [],
   "source": [
    "# \ud83d\udce4 STEP 10: EXPORT CHECKPOINTS TO S3 (AFTER TRAINING)\n",
    "print(\"\ud83d\udce4 EXPORTING CHECKPOINTS TO S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "work_dir = CHECKPOINT_DIR / \"work_dirs\"\n",
    "checkpoints = sorted(work_dir.glob(\"**/backbone.pth\"), key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"\\n\u2705 Found {len(checkpoints)} checkpoint(s)\\n\")\n",
    "    \n",
    "    # Upload all checkpoints to S3\n",
    "    s3_base = \"s3://data-labeling.livereachmedia.com/models/reidnet_v3/checkpoints/\"\n",
    "    \n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = ckpt.stat().st_size / 1e6\n",
    "        s3_path = f\"{s3_base}{ckpt.parent.name}_{ckpt.name}\"\n",
    "        \n",
    "        print(f\"\ud83d\udce6 Uploading {ckpt.name} ({size_mb:.1f} MB)...\")\n",
    "        !aws s3 cp {ckpt} {s3_path} --only-show-errors\n",
    "        print(f\"\u2705 Uploaded to {s3_path}\\n\")\n",
    "    \n",
    "    # Upload latest checkpoint with special name\n",
    "    latest = checkpoints[-1]\n",
    "    s3_latest = f\"{s3_base}reidnet_v3_latest.pth\"\n",
    "    print(f\"\ud83d\udce6 Uploading latest checkpoint as reidnet_v3_latest.pth...\")\n",
    "    !aws s3 cp {latest} {s3_latest} --only-show-errors\n",
    "    print(f\"\u2705 Latest checkpoint: {s3_latest}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\u2705 ALL CHECKPOINTS UPLOADED TO S3\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No checkpoints found. Training may still be in progress.\")\n",
    "    print(f\"   Check: {work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## \ud83d\udd27 Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "Reduce `batch_size` in config:\n",
    "- A100 (80GB): 512\n",
    "- A100 (40GB): 256\n",
    "- V100 (32GB): 128\n",
    "- V100 (16GB): 64\n",
    "- T4 (16GB): 32-64\n",
    "\n",
    "Or disable mixed precision: `config.fp16 = False`\n",
    "\n",
    "### Slow Training\n",
    "- Increase `num_workers` (try 8-16)\n",
    "- Enable DALI if available: `config.dali = True`\n",
    "- Check if dataset is on SSD\n",
    "\n",
    "### Dataset Format\n",
    "RecordIO format requires:\n",
    "- `train.rec` - binary record file\n",
    "- `train.idx` - index file\n",
    "- `property` - metadata (2 lines: num_classes,112,112 and num_images)\n",
    "\n",
    "**Note**: PyTorch ArcFace has native RecordIO reader - no MXNet required!\n",
    "\n",
    "### CUDA Errors\n",
    "```bash\n",
    "# Check CUDA installation\n",
    "!nvidia-smi\n",
    "!nvcc --version\n",
    "\n",
    "# Reinstall PyTorch with correct CUDA version\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "## \ud83d\udcd6 References\n",
    "\n",
    "- [InsightFace GitHub](https://github.com/deepinsight/insightface)\n",
    "- [ArcFace Paper](https://arxiv.org/abs/1801.07698)\n",
    "- [PyTorch ArcFace Docs](https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}